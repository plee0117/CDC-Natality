# -*- coding: utf-8 -*-
"""NICU_wolabor2018_classweight.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13JgePSLshuCPnNLzXdqEHbNM_fIEgr3L
"""

#import modules
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn.model_selection as ms
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn import ensemble
import xgboost
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

#from google.colab import drive
#drive.mount('/content/drive')

#Import dataset
#cdc2018 = pd.read_csv('CSV2018.csv')
wolabor2018 = pd.read_csv('cdc2018wolabor.csv')

#get rid of 'U' of AB_NICU
wolabor2018 =  wolabor2018[wolabor2018['AB_NICU'].isin(['Y', 'N'])]

def add_random_column_to_df (dataframe):
    import random
    mylist = []
    for i in range(0, dataframe.shape[0]):
        x = random.randint(1,1000)
        mylist.append(x)
    dataframe['RANDOM'] = mylist

    return dataframe

wolabor2018 = add_random_column_to_df(wolabor2018)

# function to split out holdout test set
def split_sets(dataframe, seed, test_prop=0.1): 
    '''
    - A function that splits specifically a dataframe into a train and test portion
    - Requires multiple assignment: train, test
    ---------------
    - dataframe: dataframe to be split
    - seed: set seed for reproducability
    - test_prop: takes a float - proportion of dataframe that should be allocated to the test set
    '''

    np.random.seed(seed)
    testIdxes = np.random.choice(range(0,dataframe.shape[0]), size=round(dataframe.shape[0]*test_prop), replace=False)
    trainIdxes = list(set(range(0,dataframe.shape[0])) - set(testIdxes))

    train = dataframe.iloc[trainIdxes,:]
    test  = dataframe.iloc[testIdxes,:]
    
    return train, test

train, test = split_sets(wolabor2018, 0, test_prop=0.1)

print(train.shape)
print(test.shape)

dsample = train.copy()

#LabelEncoding Function. Thanks Ira!
def LabelEncoding(dataframe):
    '''
    Function that takes a dataframe and transforms it with label encoding on all the categorical features.
    '''
    
    #create a list using object types since dataframe.dtypes.value_counts() only shows objects and int64
    objlist = list(dataframe.select_dtypes(include=['object']).columns)
    
    #change type then transform column using cat codes
    for col in objlist:
        dataframe[col] = dataframe[col].astype('category')
        dataframe[col] = dataframe[col].cat.codes
    
    return dataframe

#Label Encoded
dsample = LabelEncoding(dsample)
dtest = LabelEncoding(test)

X_train = dsample.drop('AB_NICU', axis=1)
y_train = dsample['AB_NICU']
X_test = dtest.drop('AB_NICU', axis=1)
y_test = dtest['AB_NICU']
y_test.value_counts()
104169*100/(1035255+104169)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

#XGBoost initial fit 
xgb = XGBClassifier()
xgb.set_params(random_state=0)
xgb.fit(X_train, y_train)
print("The training error is: %.5f" % (1 - xgb.score(X_train, y_train)))
print("The test error is: %.5f" % (1 - xgb.score(X_test, y_test)))

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, xgb.predict(X_test))

#XGBoost initial fit weighted
xgb_1 = XGBClassifier()
xgb_1.set_params(random_state=0, scale_pos_weight = 9.1)
xgb_1.fit(X_train, y_train)
print("The training error is: %.5f" % (1 - xgb_1.score(X_train, y_train)))
print("The test error is: %.5f" % (1 - xgb_1.score(X_test, y_test)))

confusion_matrix(y_test, xgb_1.predict(X_test))

# Commented out IPython magic to ensure Python compatibility.
# set the parameter grid
xgb_param_grid ={'learning_rate': [0.01,0.05],
                 'max_depth': [4,5],
                 'min_child_weight': [4,5],
                 'n_estimators': [200,300]}

#grid search
grid_search_xgb = GridSearchCV(xgb_1, xgb_param_grid, scoring='precision', cv= 5, n_jobs=-1, return_train_score = True)
# %time grid_search_xgb.fit(X_train, y_train)

# get the best parameters
print(grid_search_xgb.best_params_)
print(grid_search_xgb.best_score_)

# get the training/test errors
print("The training error is: %.5f" % (1 - grid_search_xgb.best_estimator_.score(X_train, y_train)))
print("The test error is: %.5f" % (1 - grid_search_xgb.best_estimator_.score(X_test, y_test)))

confusion_matrix(y_test, xgb.best_estimator_.predict(X_test))

# Commented out IPython magic to ensure Python compatibility.
# set the parameter grid
xgb_param_grid ={'learning_rate': [0.01,0.05],
                 'max_depth': [5, 6],
                 'min_child_weight': [3,4,5],
                 'n_estimators': [200,300,400]}

#grid search
grid_search_xgb1 = GridSearchCV(xgb, xgb_param_grid, scoring='f1_weighted', cv= 5, n_jobs=-1, return_train_score = True)
# %time grid_search_xgb1.fit(X_train, y_train)

# get the best parameters
print(grid_search_xgb1.best_params_)
print(grid_search_xgb1.best_score_)

# get the training/test errors
print("The training error is: %.5f" % (1 - grid_search_xgb1.best_estimator_.score(X_train, y_train)))
print("The test error is: %.5f" % (1 - grid_search_xgb1.best_estimator_.score(X_test, y_test)))

#Prediction with tuned hyperparameters
grid_xgb_pred = grid_search_xgb.predict(X_test)
grid_xgb_pred

grid_xgp_train_predprob = grid_search_xgb.predict_proba(X_train)[:,1]
grid_xgp_test_predprob = grid_search_xgb.predict_proba(X_test)[:,1]
#print(grid_xgp_test_predprob, grid_xgb_pred)

# Get numerical feature importances
importances_xgb = list(xgb.feature_importances_)

# List of tuples with variable and importance
feature_importances_xgb = [(feature, round(importance, 5)) for feature, importance in zip(X_train.columns, importances_xgb)]

# Sort the feature importances by most important first
xgb_feature_importances = sorted(feature_importances_xgb, key = lambda x: x[1], reverse = True )

# Print out the feature and importances 
[print('Variable: {:10} Importance: {}'.format(*pair)) for pair in xgb_feature_importances]

xgb_feature_importances_top20 = xgb_feature_importances[:20]
featureNames, featureScores = zip(*list(xgb_feature_importances_top20))
xgb_feature_importances_top20

plt.barh(range(len(featureScores)), featureScores, tick_label=featureNames)
plt.gca().invert_yaxis()
plt.title('feature importance')
plt.ylabel('Features')
plt.xlabel('Importance Score')
plt.title('Feature Importances')
plt.savefig('xgbFI.png')

# Get numerical feature importances
importances_grid_search_xgb = list(grid_search_xgb.best_estimator_.feature_importances_)

# List of tuples with variable and importance
feature_importances_xgb = [(feature, round(importance, 5)) for feature, importance in zip(X_train.columns, importances_grid_search_xgb)]

# Sort the feature importances by most important first
xgb_feature_importances = sorted(feature_importances_xgb, key = lambda x: x[1], reverse = True )

# Print out the feature and importances 
[print('Variable: {:10} Importance: {}'.format(*pair)) for pair in xgb_feature_importances]

#xgb_params_tuned_model = grid_search_xgb.best_estimator_
#xgb_feature_importance = 100.0 * (xgb_params_tuned_model.feature_importances_ / xgb_params_tuned_model.feature_importances_.max())
#xgb_important_features = X_train.columns[xgb_feature_importance >= 0.02]
#xgb_unimportant_features = X_train.columns[xgb_feature_importance < 0.02]

#confusion matrix
cm_train = confusion_matrix(y_train, grid_search_xgb.best_estimator_.predict(X_train))
print(cm_train)

cm_test = confusion_matrix(y_test, grid_search_xgb.best_estimator_.predict(X_test))
print(cm_test)

import pickle
# save model to file
pickle.dump(xgb, open("2018beforelabor_90_10_classweight.pickle.dat", "wb"))
pickle.dump(grid_search_xgb, open("2018beforelabor_90_10_classweight_grid_search_f1_weighted.pickle.dat", "wb"))

# load model later
loaded_model = pickle.load(open("2018beforelabor_90_10_classweight.pickle.dat", "rb"))

loaded_model.predict(X_test)