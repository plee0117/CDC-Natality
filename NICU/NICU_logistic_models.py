# -*- coding: utf-8 -*-
"""Logistic_Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tYMnBgmFVvmZ1NRFHZptJDGCQpF-mxOu
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
plt.style.use('ggplot')
import seaborn as sns
from scipy import stats
from scipy.stats import chi2_contingency
pd.set_option('display.max_columns', 100)
from sklearn import linear_model
from sklearn import metrics
from statsmodels.discrete.discrete_model import LogitResults
from statsmodels.discrete.discrete_model import Logit
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from statsmodels.tools.tools import add_constant
import re

"""### Functions"""



#pull in function to dummify columns
def dummify_columns(dataframe,var_list):
    '''
    dummifies a columns, merges with the dataframe, and drops the non-dummified column
    ------------
    dataframe: full dataframe
    variable: list of column names as string
    '''
    for vr in var_list:
        dummified_feature = pd.get_dummies(dataframe[vr], prefix=vr,drop_first=True)
        dataframe = pd.concat([dataframe,dummified_feature],axis=1,sort='False')
    dataframe = dataframe.drop(var_list, axis=1)
    return dataframe

def xy_split(dataframe,target):
    '''
    splits a dataframe into a target array and estimator dataframe
    '''
    y=dataframe[target]
    X=dataframe.drop(target, axis=1)
    return X,y

def standardize_columns(df,var_list):
    '''
    standardize a columns, merges with the dataframe, and drops the non-standardized column
    ------------
    dataframe: full dataframe
    variable: column name as string
    '''
    from sklearn.preprocessing import StandardScaler
    scaleit = StandardScaler()
    for vr in var_list:
        scaled_feature = pd.DataFrame(scaleit.fit_transform(df[[vr]]), index = df.index, columns=[vr+'__S'])
        df = pd.concat([df,scaled_feature],axis=1,sort=False)
    df.drop(columns = var_list, inplace = True)
    return df

def basic_significance(dataframe, list_to_dummify, target):
    '''
    fits a non-regularized logistic model to target using dataframe predictors
    prints model accuracy and outputs significant coefficients order by absolute magnitude
    ----------
    list_to_dummify: a list of columns in string format that require dummification before modeling
    '''
    #process the dataframe
    df = dataframe.copy()
    df = dummify_columns(df, list_to_dummify)
    X,y = xy_split(df, target)
    X = add_constant(X)
    y = y.replace('N',0).replace('Y',1)
    print(y.describe())
    print(X.dtypes)
    #fit the model
    logit = Logit(y,X)
    fitted_logit = Logit.fit(logit)
    #store accuracy
    c_mat = confusion_matrix(y, np.round(Logit.predict(logit, fitted_logit.params)))
    accuracy = sum(c_mat.diagonal())/np.sum(c_mat)
    print('model train accuracy: %s' %(accuracy))
    #store significant coefs
    coefs = pd.DataFrame(fitted_logit.pvalues[fitted_logit.pvalues<0.05])
    coefs['coefs'] = fitted_logit.params.filter(items=coefs.index)
    coefs.columns = ['p-values', 'coefs']
    coefs['abs_coefs'] = np.abs(coefs.coefs)
    coefs = coefs.sort_values(by='abs_coefs', ascending = False)
    coefs = coefs.drop('abs_coefs', axis =1)
    return fitted_logit, coefs

def forward_selection(dataframe, target, list_to_dummify, criteria='bic'):
    '''
    runs forward selection process to select best predictor set based on bic or aic
    returns a dictionary with the variable set and aic/bic at each step
    ----------
    criteria: default value bic, otherwise aic is used
    list_to_dummify: a list of columns in string format that require dummification before modeling
    '''
    #create target array, intercept only dataframe, and list of variables to select from
    X = pd.DataFrame()
    y = dataframe[target]
    X['const'] = np.ones(cchd.shape[0])
    var_list = list(dataframe.columns)
    var_list.remove(target)
    
    #create empty dictionary to store output of each step
    models = {'model_vars': [], 'scoring_crit':[]}
    
    #define while loop that will run until all variables have been selected
    while len(var_list) > 0: 
        
        #define empty list to store aic/bic values temporarily for step attempt
        crit_vals = []
        
        #try adding variables one by one find lowest vif model for current step
        for var in var_list:
            #create temporary df with all previously selected variables + the new variable being tried
            tempX=pd.concat([X,dataframe[var]],axis=1)
            #dummify the variable if necessary
            if var in list_to_dummify:
                tempX = dummify_columns(tempX, [var])
            #fit the logistic model
            logit = Logit(y,tempX)
            fitted_logit = Logit.fit(logit)
            #store aic or bic in a list for each variable attempted
            if criteria == 'bic':
                crit_vals += [fitted_logit.bic]
            else:
                crit_vals += [fitted_logit.aic]
        
        #find the index of the lowest bic model and store the name of the variable which produced it
        min_crit_idx = crit_vals.index(min(crit_vals))
        best_var = var_list[min_crit_idx]
        
        #add the best variable to the df
        X = pd.concat([X, dataframe[best_var]], axis=1)
        
        #store the variables and aic/bic for the best model at the current step
        models['model_vars']+=[list(X.columns)]
        models['scoring_crit']+=[min(crit_vals)]
        
        #dummify the added variable if necessary
        if best_var in list_to_dummify:
            X = dummify_columns(X, [best_var])
        
        #remove the added variable from the variable list and track progress
        var_list.remove(best_var)
        print('adding var: %s' %(best_var))
        
    return models

def best_forward_set(forward_models):
    '''
    returns cleaned columns as they appear in the original dataset which are used in best forward selection model
    ----------
    forward_models: dictionary output by the forward_selection function
    '''
    model_idx = forward_models['scoring_crit'].index(min(forward_models['scoring_crit']))
    best_cols = forward_models['model_vars'][model_idx]
    best_cols_clean = []
    for i in range(0,len(best_cols)):
        best_cols_clean+=[re.search('\D+', best_cols[i])[0]]
    final_cols=[]
    for i in range(0,len(best_cols_clean)):
        final_cols+=[re.sub("_$", "", best_cols_clean[i])]
    final_cols2=[]
    for i in range(0,len(final_cols)):
        final_cols2+=[re.sub("_(?:Y|M|N|)$", "", final_cols[i])]
    final_cols_clean = set(final_cols2)
    final_cols_clean.remove('const')
    return list(final_cols_clean)

def grid_search_logit(dataframe, columns_to_dummify, target, grid_params, standardize = 'Y'):
    '''
    fit regularized logistic model with grid search to select optimal reg paramter
    returns score, parameters, and coefficients from best model
    ----------
    columns_to_dummify: a list of columns in string format that require dummification before modeling
    grid_params: parameters to grid search across
    '''
    df = dataframe.copy()
    df = dummify_columns(df, columns_to_dummify)
    X,y = xy_split(df, target)
    if standardize == 'Y':
        X = standardize_columns(X,list(X.columns))
    logit = linear_model.LogisticRegression()
    logit.set_params(solver='liblinear')
    log_grid = GridSearchCV(estimator = logit, param_grid=grid_params, scoring='accuracy', cv=5, return_train_score=True)
    log_grid.fit(X,y)
    coefs = pd.Series([item for sublist in log_grid.best_estimator_.fit(X,y).coef_ for item in sublist], index=X.columns)
    order = abs(coefs).sort_values(ascending=False)
    return log_grid.best_score_, log_grid.best_params_, coefs[order.index], log_grid.best_estimator_

def grid_search_logitw(dataframe, columns_to_dummify, target, grid_params, standardize = 'Y'):
    '''
    fit regularized logistic model with grid search to select optimal reg paramter
    returns score, parameters, and coefficients from best model
    ----------
    columns_to_dummify: a list of columns in string format that require dummification before modeling
    grid_params: parameters to grid search across
    '''
    df = dataframe.copy()
    df = dummify_columns(df, columns_to_dummify)
    X,y = xy_split(df, target)
    if standardize == 'Y':
        X = standardize_columns(X,list(X.columns))
    logit = linear_model.LogisticRegression()
    logit.set_params(solver='liblinear', class_weight='balanced')
    log_grid = GridSearchCV(estimator = logit, param_grid=grid_params, scoring='precision', cv=5, return_train_score=True)
    log_grid.fit(X,y)
    coefs = pd.Series([item for sublist in log_grid.best_estimator_.fit(X,y).coef_ for item in sublist], index=X.columns)
    order = abs(coefs).sort_values(ascending=False)
    return log_grid.best_score_, log_grid.best_params_, coefs[order.index], log_grid.best_estimator_

def precision(y, X_pred):
    c = confusion_matrix(y, X_pred)
    return c[1][1]/(c[0][1]+c[1][1])







"""### All_Imputed Model"""

#redefine variable dictionary
variables = {'nominal_categorical_ndummified':['MRACEHISP','MAR_P','DMAR','MEDUC','FRACEHISP',
                                    'FEDUC','WIC','RF_PDIAB','RF_GDIAB','RF_PHYPE','RF_GHYPE',
                                    'RF_EHYPE','RF_PPTERM','RF_INFTR','RF_FEDRG','RF_ARTEC','RF_CESAR',
                                    'CA_ANEN', 'CA_CCHD', 'CA_CDH', 'CA_CLEFT', 'CA_CLPAL', 'CA_DISOR',
                                    'CA_DOWN', 'CA_GAST', 'CA_HYPO', 'CA_LIMB', 'CA_MNSB', 'CA_OMPH',
                                    'IP_GON','IP_SYPH','IP_CHLAM','IP_HEPB','IP_HEPC', 'PAY'],
             'nominal_categorical_dummified':['BMI_R', 'CIG0_R', 'CIG1_R', 'CIG2_R', 'CIG3_R', 'DPLURAL',
                                  'FAGEREC11', 'ILLB_R11', 'ILOP_R11', 'MAGER9', 'PRECARE5', 'PREVIS_REC', 
                                  'RESTATUS', 'WTGAIN_REC'],
             'continuous':['PRIORTERM','PRIORLIVE','PRIORDEAD','LBO_REC','TBO_REC','RF_CESARN'],
             'target':['AB_NICU']}

from google.colab import drive
drive.mount('/content/drive')

#load in dataframe
cchd = pd.read_csv('/content/drive/My Drive/imputed/a3nicutrain.csv')
cchd_test = pd.read_csv('/content/drive/My Drive/imputed/a3nicutest.csv')

cchd['AB_NICU'] = cchd['AB_NICU'].replace('Y',1).replace('N',0)

cchd_test['AB_NICU'] = cchd_test['AB_NICU'].replace('Y',1).replace('N',0)

cchd_test.columns

cchd_test = dummify_columns(cchd_test, variables['nominal_categorical_ndummified'])
X_test,y_test = xy_split(cchd_test, 'AB_NICU')
X_test_standardized = standardize_columns(X_test,list(X_test.columns))
X_test_standardized_constant = add_constant(X_test_standardized)
X_test_constant = add_constant(X_test)

"""#### *Basic non-regularized model for feature significance*"""

y_test

#run non-regularized model with all features
#output all siginificant features ordered by absolute coefficient value

f_logit_b, coefs_b = basic_significance(cchd, variables['nominal_categorical_ndummified'], 'AB_NICU')
coefs_b
y_test = y_test.replace('Y',1).replace('N',0f)
print('precision: %s' %(precision(y_test, np.round(f_logit_b.predict(X_test_constant)))))

"""#### *Forward selection for feature significance*"""

#find best model via forward selection
forward_models = forward_selection(cchd, 'AB_NICU', variables['nominal_categorical_ndummified'], criteria='bic')

def best_forward_set___(forward_models):
    '''
    returns cleaned columns as they appear in the original dataset which are used in best forward selection model
    ----------
    forward_models: dictionary output by the forward_selection function
    '''
    model_idx = forward_models['scoring_crit'].index(min(forward_models['scoring_crit']))
    best_cols = forward_models['model_vars'][model_idx]
    # best_cols_clean = []
    # for i in range(0,len(best_cols)):
    #     best_cols_clean+=[re.search('\D+', best_cols[i])[0]]
    # final_cols=[]
    # for i in range(0,len(best_cols_clean)):
    #     final_cols+=[re.sub("_$", "", best_cols_clean[i])]
    # final_cols2=[]
    # for i in range(0,len(final_cols)):
    #     final_cols2+=[re.sub("_(?:Y|M|N|)$", "", final_cols[i])]
    # final_cols_clean = set(final_cols2)
    # final_cols_clean.remove('const')
    # return list(final_cols_clean)
    return best_cols

asdf = best_forward_set___(forward_models)

best = ['DPLURAL', 'PREVIS_REC', 'RF_GHYPE', 'RF_PPTERM', 'ILLB_R11', 'RF_PHYPE', 'RF_PDIAB', 'WTGAIN_REC',
 'RF_CESAR', 'CIG3_R', 'CA_CCHD', 'RF_GDIAB', 'PRIORTERM', 'PRECARE5', 'FAGEREC11', 'CA_GAST', 'CA_DOWN', 'RESTATUS', 'PAY',
 'RF_EHYPE', 'CA_DISOR', 'MRACEHISP', 'IP_HEPC', 'MAGER9', 'CA_MNSB', 'CA_CDH', 'FEDUC',
 'CIG2_R', 'CA_CLEFT', 'CA_OMPH', 'CA_CLPAL', 'RF_INFTR', 'DMAR', 'IP_SYPH', 'MEDUC',
 'CA_HYPO', 'lrg_miss_imp', 'PRIORDEAD', 'WIC', 'RF_CESARN', 'PRIORLIVE', 'FRACEHISP', 'CA_LIMB', 'CIG1_R', 'TBO_REC', 'CA_ANEN', 'MAR_P', 'IP_GON',
 'BMI_R']

#view coefficients and accuracy on best forward selected model

cchd2 = cchd[best+['AB_NICU']]
f_logit_bf, coefs_bf= basic_significance(cchd2,set(best)&set(variables['nominal_categorical_ndummified']), 'AB_NICU')
coefs_bf

coefs_bf.sort_values(by = 'p-values',ascending=True).head(25)

coefs_bf.to_csv('nicu_logit.csv')

cchd_test = pd.read_csv('Datasets/cchd_allimp_test.csv')
cchd_test = cchd_test.drop('Unnamed: 0', axis=1)
cchd_test_best = cchd_test[best+['AB_NICU']]
cchd_test_best = dummify_columns(cchd_test_best, ['RF_PDIAB', 'RF_GDIAB', 'MRACEHISP', 'RF_GHYPE'])
X_test_best,y_test_garbage = xy_split(cchd_test_best, 'AB_NICU')
X_test_best_standardized = standardize_columns(X_test_best,list(X_test.columns))
X_test_best_standardized_constant = add_constant(X_test_best_standardized)
X_test_best_constant = add_constant(X_test_best)

print('precision: %s' %(precision(y_test, np.round(f_logit_bf.predict(X_test_best_constant)))))

"""#### *Grid search model with all estimators for prediction*"""

#develop predictive logistic model with all predictors
params = {'C':np.logspace(-4,4, 20)}
acc, params, coefs, b_estimator = grid_search_logit(cchd, variables['nominal_categorical_ndummified'], 'AB_NICU', params, standardize = 'N')
print('test accracy: %s' %(acc))
print('params: %s' %(params))
print('validation accuracy: %s' %(b_estimator.score(X_test, y_test)))
print('precision: %s' %(precision(y_test, b_estimator.predict(X_test))))
coefs

##develop predictive logistic model with all predictors standardized dataset
params = {'C':np.logspace(-4,4, 20)}
accs, paramss, coefss, b_estimators = grid_search_logit(cchd, variables['nominal_categorical_ndummified'], 'AB_NICU', params, standardize = 'Y')
print('test accracy: %s' %(accs))
print('params: %s' %(paramss))
print('validation accuracy: %s' %(b_estimators.score(X_test_standardized, y_test)))
print('precision: %s' %(precision(y_test, b_estimators.predict(X_test_standardized))))
coefs

"""#### *Grid search model with best estimators selected via forward aic*"""

#accuracy is slightly better with forward selected set of variables
#aic used as goal here is prediction oriented
#much less regularization is perfmormed in optimal mode with forward selected set of variables
forward_aic = forward_selection(cchd, 'AB_NICU', variables['nominal_categorical_ndummified'], criteria='aic')
best_aic = best_forward_set(forward_aic)
best_aic
best_aic[-1]='CIG_0'
cchd3 = cchd[best_aic+['AB_NICU']]
dummy = [var for var in best_aic if var in variables['nominal_categorical_ndummified']]
params = {'C':np.logspace(-4,4, 20)}
acc2, params2, coefs2 = grid_search_logit(cchd3,dummy, 'AB_NICU', params, standardized = 'N')
print('test accracy: %s' %(acc2))
print('params: %s' %(params2))
coefs2

#same model using standardized features
forward_aic = forward_selection(cchd, 'AB_NICU', variables['nominal_categorical_ndummified'], criteria='aic')
best_aic = best_forward_set(forward_aic)
best_aic
best_aic[-1]='CIG_0'
best_aic[10]='CIG_1'
cchd3 = cchd[best_aic+['AB_NICU']]
dummy = [var for var in best_aic if var in variables['nominal_categorical_ndummified']]
params = {'C':np.logspace(-4,4, 20)}
acc2, params2, coefs2 = grid_search_logit(cchd3,dummy, 'AB_NICU', params, standardize='Y')
print('test accracy: %s' %(acc2))
print('params: %s' %(params2))
coefs2

"""### Cont_only_Imputed Model"""

#if time - focus on SVM first

#other to try - only cont imputed model, upsample with matching downsample,
#model on 10% of data with different test set with and without specifying class weights
#much later - customizing loss function

